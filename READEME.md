
<div align="center">
  <img src="public/scrapper-icon.png" alt="UFRGS Scraper" width="200"/>
</div>

# UFRGS Admissions Data Scraper

A Python-based web scraping tool that collects and processes admissions data from UFRGS (Universidade Federal do Rio Grande do Sul) for both Vestibular and SISU selection processes.

## Features

- ğŸ“ Scrapes admissions data from UFRGS portal
- ğŸ“Š Supports both Vestibular and SISU selection processes
- ğŸ—„ï¸ Automatic database storage with batch processing
- ğŸ³ Fully containerized with Docker
- ğŸ“… Historical data collection (2016-2025)
- ğŸ” Course filtering capabilities
- ğŸ”„ Data transformation and normalization

## Project Structure

```
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api_wrapper.py       # HTTP request handling
â”‚   â””â”€â”€ database_service.py  # Database operations
â”œâ”€â”€ main.py                  # Main scraper script
â”œâ”€â”€ Dockerfile              # Container configuration
â”œâ”€â”€ docker-compose.yml      # Multi-container setup
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ run.sh                  # Execution script
â””â”€â”€ .env                    # Environment variables
```

## Prerequisites

- Docker and Docker Compose
- Python 3.11+ (if running locally)

## Installation & Setup

### Using Docker (Recommended)

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd ufrgs-scraper
   ```

2. **Build the Docker image**
   ```bash
   docker build -t scraper .
   ```

3. **Configure environment variables**
   Create a `.env` file with your database configuration:
   ```env
   DATABASE_URL=your_database_url_here
   # Add other required environment variables
   ```

### Local Development

1. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your configuration
   ```

## Usage

### Using the Shell Script (Recommended)

The easiest way to run the scraper is using the provided shell script:

### Manual Docker Execution

```bash
# Scrape data for a specific course across all years
docker run --rm scraper python main.py "course_name" 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025

# Scrape data for specific years only
docker run --rm scraper python main.py "psicologia" 2023 2024 2025

# Scrape all courses for specific years
docker run --rm scraper python main.py "" 2024 2025
```

### Local Execution

```bash
# Run with course filter and specific years
python main.py "course_name" 2023 2024 2025

# Run with course filter for all years
python main.py "psicologia"
```

## How It Works

1. **Course Discovery**: The scraper first fetches available courses for each year and selection type (Vestibular/SISU)
2. **Data Extraction**: For each course, it retrieves candidate data including rankings, scores, and status information
3. **Data Processing**: Raw HTML tables are parsed and converted to structured data
4. **Data Storage**: Processed data is stored in the database using batch operations

## Data Fields

The scraper collects and normalizes the following candidate information:

| Original Field | Normalized Field | Description |
|----------------|------------------|-------------|
| ClassificaÃ§Ã£o | classification | Candidate ranking |
| MÃ©dia | score | Average score |
| Vaga(s) de ConcorrÃªncia | concurrence_type | Competition type |
| PerÃ­odo Vaga | period | Period/shift |
| Vaga de Ingresso | enter_type | Entry type |
| SituaÃ§Ã£o | status | Current status |
| Data SituaÃ§Ã£o | date | Status date |
| Candidato | name | Candidate name |

Additional fields added during processing:
- `year`: Selection year
- `course_name`: Full course name
- `exam_type`: Selection type (vest/sisu)

## Configuration

### Environment Variables

Create a `.env` file with the following variables:

```env
DATABASE_URL=postgresql://user:password@host:port/database
# Add other configuration as needed
```

### Docker Compose

For development with database included:

```bash
docker-compose up -d
```
## Performance Considerations

- **Rate Limiting**: The scraper respects the target server by implementing appropriate delays
- **Batch Processing**: Database operations are batched for efficiency
- **Memory Management**: Large datasets are processed in chunks
- **Caching**: Course data is cached to avoid redundant requests

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Disclaimer

This tool is for educational and research purposes only. Please respect the target website's robots.txt and terms of service. The authors are not responsible for any misuse of this tool.